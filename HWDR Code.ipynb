{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NIz7s0-cRZ1",
        "outputId": "971cdfae-8ca3-4f1e-9dc2-2ffe5b18a57b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 21ms/step - accuracy: 0.6577 - loss: 1.0654 - val_accuracy: 0.9417 - val_loss: 0.2017\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.8515 - loss: 0.4758 - val_accuracy: 0.9517 - val_loss: 0.1541\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.8771 - loss: 0.3858 - val_accuracy: 0.9639 - val_loss: 0.1202\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.8934 - loss: 0.3454 - val_accuracy: 0.9679 - val_loss: 0.1094\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.9015 - loss: 0.3144 - val_accuracy: 0.9690 - val_loss: 0.0992\n",
            "Epoch 6/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - accuracy: 0.9076 - loss: 0.2953 - val_accuracy: 0.9717 - val_loss: 0.0941\n",
            "Epoch 7/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.9116 - loss: 0.2851 - val_accuracy: 0.9725 - val_loss: 0.0901\n",
            "Epoch 8/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.9132 - loss: 0.2725 - val_accuracy: 0.9762 - val_loss: 0.0804\n",
            "Epoch 9/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - accuracy: 0.9200 - loss: 0.2639 - val_accuracy: 0.9752 - val_loss: 0.0808\n",
            "Epoch 10/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.9221 - loss: 0.2465 - val_accuracy: 0.9767 - val_loss: 0.0786\n",
            "Epoch 11/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 21ms/step - accuracy: 0.9269 - loss: 0.2393 - val_accuracy: 0.9801 - val_loss: 0.0705\n",
            "Epoch 12/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.9266 - loss: 0.2335 - val_accuracy: 0.9787 - val_loss: 0.0707\n",
            "Epoch 13/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - accuracy: 0.9275 - loss: 0.2333 - val_accuracy: 0.9783 - val_loss: 0.0683\n",
            "Epoch 14/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 22ms/step - accuracy: 0.9317 - loss: 0.2243 - val_accuracy: 0.9820 - val_loss: 0.0595\n",
            "Epoch 15/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.9346 - loss: 0.2179 - val_accuracy: 0.9798 - val_loss: 0.0642\n",
            "Epoch 16/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - accuracy: 0.9340 - loss: 0.2118 - val_accuracy: 0.9797 - val_loss: 0.0644\n",
            "Epoch 17/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - accuracy: 0.9343 - loss: 0.2095 - val_accuracy: 0.9805 - val_loss: 0.0643\n",
            "313/313 - 1s - 3ms/step - accuracy: 0.9817 - loss: 0.0554\n",
            "\n",
            "Test Accuracy: 98.17%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0  # Normalize pixel values\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Add a channel dimension\n",
        "x_train = np.expand_dims(x_train, axis=-1)  # Shape: (60000, 28, 28, 1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)    # Shape: (10000, 28, 28, 1)\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "datagen.fit(x_train_split)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28, 1)),  # Updated input shape\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    datagen.flow(x_train_split, y_train_split, batch_size=32),  # Training generator\n",
        "    validation_data=(x_val, y_val),  # Validation data\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Function to preprocess and predict a custom image\n",
        "def predict_custom_image(image_path):\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "        img = ImageOps.invert(img)  # Invert colors for MNIST format\n",
        "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        # Normalize the image\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Check if the digit is centered; pad if necessary\n",
        "        img_array = np.pad(img_array, ((4, 4), (4, 4)), mode='constant', constant_values=0)\n",
        "\n",
        "        # Crop or center the digit\n",
        "        img_array = img_array[4:-4, 4:-4]\n",
        "\n",
        "        # Add channel dimension\n",
        "        img_array = img_array.reshape(1, 28, 28, 1)\n",
        "\n",
        "        # Predict the digit\n",
        "        prediction = model.predict(img_array)\n",
        "        predicted_label = np.argmax(prediction)\n",
        "\n",
        "        # Display the image and prediction\n",
        "        plt.imshow(img_array.reshape(28, 28), cmap='gray')\n",
        "        plt.title(f\"Predicted: {predicted_label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Model Prediction: {predicted_label}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}. Please ensure the file is a valid image.\")\n",
        "\n",
        "# Test on a custom image\n",
        "image_path = input(\"Enter the path to your handwritten digit image: \").strip()\n",
        "if os.path.exists(image_path):\n",
        "    predict_custom_image(image_path)\n",
        "else:\n",
        "    print(\"File not found. Please check the file path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and predict a custom image\n",
        "def predict_custom_image(image_path):\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "        img = ImageOps.invert(img)  # Invert colors for MNIST format\n",
        "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        # Normalize the image\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Check if the digit is centered; pad if necessary\n",
        "        img_array = np.pad(img_array, ((4, 4), (4, 4)), mode='constant', constant_values=0)\n",
        "\n",
        "        # Crop or center the digit\n",
        "        img_array = img_array[4:-4, 4:-4]\n",
        "\n",
        "        # Add channel dimension\n",
        "        img_array = img_array.reshape(1, 28, 28, 1)\n",
        "\n",
        "        # Predict the digit\n",
        "        prediction = model.predict(img_array)\n",
        "        predicted_label = np.argmax(prediction)\n",
        "\n",
        "        # Display the image and prediction\n",
        "        plt.imshow(img_array.reshape(28, 28), cmap='gray')\n",
        "        plt.title(f\"Predicted: {predicted_label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Model Prediction: {predicted_label}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}. Please ensure the file is a valid image.\")\n",
        "\n",
        "# Test on a custom image\n",
        "image_path = input(\"Enter the path to your handwritten digit image: \").strip()\n",
        "if os.path.exists(image_path):\n",
        "    predict_custom_image(image_path)\n",
        "else:\n",
        "    print(\"File not found. Please check the file path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "eaN1Gz8sB5Ry",
        "outputId": "6d827aa8-99ae-4791-898e-66a7ae7b0d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your handwritten digit image: /content/image_2024-11-23_004926022.png\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Prediction: 5\n"
          ]
        }
      ]
    }
  ]
}